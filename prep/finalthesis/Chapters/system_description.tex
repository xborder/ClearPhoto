
% ===============================================================
% =						System Description						=
% ===============================================================
\chapter{System Description and Functionalities}
\label{cha:system}
This chapter describes the architecture and general structure of the system, as well as a detailed explanation about the functionalities implemented with the proposed technologies and interface displayed.

\section{System Description}
\label{sec:system_description}


\section{Functionalities}
\label{sec:functionalities}

This section describes in detail the functionalities and experiments implemented in our system. The set of functionalities implemented were considered to be more interesting to explore in a real-time environment than the usual set of features already presented in regular camera applications or digital cameras.

\subsection{Colour Histograms and Average Saturation}
\label{sub:histograms}

Colour histograms have been used for a long time in image processing and photography to measure the distribution of colours. A colour histogram represents the number of pixels in each of a fixed list of colour ranges, that span the image's color space, the set of all possible colors.

These can be built in any kind of color space and have multiple dimensions depending on the number of measurements taken, although it is normally used to count the number of pixels for three-dimensional colour spaces.

The visualization of such information becomes really important when colour is one of the elements in which photography is most focused on. \citeauthor{bertin1983semiology} stated that in the field of information visualization, colour has historically been used as one of the primary visual variables through which difference in data can be distinguished and is considered as one of the fundamental building blocks of visualizations today.

Exploring the importance of colour, in \cite{haber2011colourvis} colour information is visualized with a variation of a stacked line graph visualization, which give the viewer a sense of the colours and the amount of each colour that make up either an image, or a series of images. The author uses mainly the \emph{hue} channel in the \emph{HSB} colour space. When using one image, it uses a six-colour labelled histogram to define an image as shown in Figure \ref{fig:colourvis1}. However, even though the histogram is simplified to a six colour range, it is difficult to compare the histograms generated by two different images. Figure \ref{fig:colourvis2} shows the application of a labelled stack line graph when comparing two images.

\begin{figure}[htbp]
	\centering
    \subfigure[] {
                \includegraphics[width=0.3\textwidth]{interface/colour_histogram/colourvis1_o.jpg}
                \label{fig:hough_sinusoidal}
    }
    \subfigure[] {
                \includegraphics[width=0.3\textwidth]{interface/colour_histogram/colourvis1.jpg}
                \label{fig:multiple_sinusoidal}
    }
  \caption{"Fishing in Spring, the Pont de Clichy" by Vincent van Gogh (a) and the corresponding labelled linear histogram representation.}
  \label{fig:colourvis1}
\end{figure}


\begin{figure}[htb]
	\centering
    \subfigure[] {
                \includegraphics[width=0.3\textwidth]{interface/colour_histogram/colourvis2_o.jpg}
                \label{fig:hough_sinusoidal}
    }
    \subfigure[] {
                \includegraphics[width=0.4\textwidth]{interface/colour_histogram/colourvis2.jpg}
                \label{fig:multiple_sinusoidal}
    }
  \caption{Linear histograms representations of “The Funeral of the Anarchist Galli” by Carlo Carrà (top) and “Speed+Sound” by Giacomo Balla (bottom) (a) with a labelled stacked line graph representation comparing the amount of each colour in both paintings (b).}
  \label{fig:colourvis2}
\end{figure}

Another important property is a colour saturation which is the colourfulness of a colour relative to its own brightness. The saturation of a color is determined by a combination of light intensity and how much it is distributed across the spectrum of different wavelengths. The purest (most saturated) color is achieved by using just one wavelength at a high intensity, such as in laser light. If the intensity drops, then as a result the saturation drops.

Colour saturation can the influence the vividness of an image as a desaturated image is said to be dull, less colourful or washed out but can also make the impression of being softer. 


\subsubsection{Algorithm Description}

As described in previously, colour is obviously a very important part in an image. With that being said, we implemented some simple algorithms that compute the amount of colour and saturation that exists in an image.

It was already said that an histogram is important way to visualize the amount of each colour represented on the scenario. We also computed a couple of histograms to give this information. For our first implementation, we generated four histograms for each channel of the input image. Each frame to be processed comes in a packed format and is then converted both to a gray image and to a \emph{RGB} image. We then create a full-range histogram for each of the channels.

Knowing the amount of each colour in each one of the channels we can then obtain an interval that contains all the relevant colours. We considered as relevant colours, colours that have an amount higher than 5\% of the maximum value. Since all histograms are normalized to a range of $[0,255]$, colours with a count higher than 12, are considered relevant. Given the colour range for each channel, we also calculate the amount of pixels that are near the limits. This amount is calculated by applying a Gaussian function where the peak is each one of the limits in the colour range. This can be translated by the equation
\begin{equation}
	\sum_{i \exists [0,50]}^{i}hist(i) + hist(i)*\frac{1}{0.7\sqrt{2\pi}}e^{-i},
	\label{eq:hist1}
\end{equation}
\begin{equation}
	\sum_{j \exists [205,255]}^{j}hist(j) + hist(j)*\frac{1}{0.7\sqrt{2\pi}}e^{-j}
	\label{eq:hist2}
\end{equation}
where the Gaussian function has the parameters $\sigma = 0.7, \sigma^{2} = 0.5$ and $\mu = 0$. $i$ and $j$ are the colour value in the histogram and $hist(i)$ is the amount of colour on the frame. The result from equations \ref{eq:hist1} and \ref{eq:hist2} are then used to represent visually the amount of pixels near each limit. 

In our second implementation we did another histogram but in this case we converted the input image to the \emph{HSV} colour space and performed an histogram over the \emph{Hue} channel. Due to the chosen method to visualize the \emph{Hue} spectrum was quantized into six bins, which correspond to the primary colours red, green and blue, and its consequent secondary colours.
When processing the input image, a vector with a total length equal to the number of bins, would be accumulated depending on the colour of the pixel, since each bin covered a total of thirty colours with the \emph{Hue} spectrum. The amount percentage for each bin is then calculated and displayed to the user.

As referred in the previous section, the colourfulness of a scenario is always important considering that its what makes an image more vivid. Due to that fact, we also implemented a low saturation detector. This detector works by averaging the \emph{Saturation} channel in the \emph{HSV} colour space and compare its value to a threshold. If the value of saturation of an image is below a threshold of 90, then a suggestion is shown meaning that the use of a monochromatic filter might be useful in that situation. This threshold was defined by comparing the average saturation of images labelled as low-saturation images in the Flickr.com community.

\subsubsection{Interface Display}
In the previous section we described the algorithms for a saturation detector and two similar ways to calculate the colours presented in an image. For the later, it becomes even more important to correctly visualize that information.

For our first implementation which involved the calculation of three histograms based in the \emph{RGB} colour space and one with the image converted to gray scale, we tried to represent this information by using less space than a conventional histogram and by slightly simplifying the amount of information. As shown in Figure X, our histograms are visualized by a bar with a dynamic size within a certain boundaries which define the total range of colours within each channel. The purpose of this representation for the user to have a perception the amount of colours being used in each channel. The size of the bar is dynamic since it is defined by the first and last relevant colours found in a channel.

As mentioned before, this representation also shows the amount of pixels found near the boundaries of the colour range of each channel. This is represented by a line that grows vertically depending on the amount calculated by the equations \ref{eq:hist1} and \ref{eq:hist2}.


For the second implementation of an histogram, we chose to represent the \emph{Hue} concentration of each bin through a hexagon that resembles a colour wheel divided into six colours. Each section of the hexagon is filled with the percentage amount of the respective colour presented in the input image as shown in the Figure x.

For the saturation detection, we only display an icon as an indicator that the usage of a monochromatic filter might be useful in that situation (Figure x).


\subsubsection{Discussion}

After implementing these features, we could conclude a couple of things. Starting by our first implementation of a histogram, the first noticeable problem is the fact that the histogram is made for the \emph{RGB} colour space. Although it is the most used colour space in image manipulation applications and photography systems, the conversion of real-world colours to \emph{RGB} is not direct.

With the representation we created a one-dimensional histogram that omitted the amount of each colour, to show a range of colours being used in the image. This range is determined by the first and last colours found as relevant with the purpose to show the spread of tones across a channel. As a general rule, is most cases a nice balanced shot as nice spread of tones which peak somewhere around the middle and taper off around the edges. Ignoring the peak around the middle and stabilization around the edges, we think this representation successfully serves the purpose giving an idea of the colours being used. The problem with this representation is that it does not contemplate any flaws in the middle of the histogram. This means that if it shows that a range of colours is being used,within that range, there's a possibility that some of the colours were not found in the image. Figure \ref{fig:disc_hist} illustrates this example. Imagining that the blue channel of an image as the histogram shown in Figure \ref{fig:disc_hist1}, there is a concavity between two peaks that contains colours that weren't used. Figure \ref{fig:disc_hist2} displays our representation based on that histogram, where we can see that the concavity in the original histogram and the colours that weren't used in that concavity, are shown as a part of the range of colours detected.

\begin{figure}[htb]
	\centering
    \subfigure[] {
                \includegraphics[width=0.3\textwidth]{interface/colour_histogram/hist1_ex.png}
                \label{fig:disc_hist1}
    }
    \subfigure[] {
                \includegraphics[width=0.3\textwidth]{interface/colour_histogram/hist2_ex.png}
                \label{fig:disc_hist2}
    }
  	\caption{Illustrative figure of the problem found in our histogram representation (b) compared to the correspondent regular histogram (a).}
	\label{fig:disc_hist}
\end{figure}

Our second implementation using an hexagon in resemblance of a colour wheel, it is easier to convert real-world colours to the \emph{HSV} colour space. Although it is easier to perceive the colours being used, it does not have into account the saturation or brightness of each colour. 
The main problem in this representation is the amount percentages displayed. As it is, each part of the hexagon will be fully filled when all the \emph{Hue} values in the converted image are within the same range, which results in a scaling problem as this rarely happens. Each processed frame normally contains at least a small portion of all the colours presented in the wheel, meaning that with the representation chosen, small amounts of each section will be filled making them almost irrelevant. Even with this problem, the main purpose of this representation is to visualize the colours that have a higher presence in a scenario and what complementary colour should be used to provide a colour balance (Section \ref{subsub:colour_balance}), which we think it really serves the purpose.


As for our saturation detection method it is not very complex, however the threshold chosen to classify an input frame as highly saturated or not, was selected by averaging the saturation of a set of images labelled as \emph{low saturation} by the Flickr.com's community. This being said the threshold was calculated over a set of static images which may have been digitally manipulated, which means that in a real-time scenario this threshold might not be the most indicated as it is difficult to find scenarios to test and calculate a more adequate threshold.

\subsection{Colour Templates and Hue Counting}
\label{sub:color}

\subsection{Face Detection and Composition Guidelines}
\label{sub:face_guidelines}

\subsection{Object Segmentation}
\label{sub:segmentation}

Unless it is a photography of a seascape or a landscape, it is normal for a photo to have a relevant subject. With this in mind, segmentation of a subject in multiple scenarios might be useful to attain a better composition.

Segmentation of an object in a photo, is a topic that as already been widely researched. One of the simplest ways of extracting information about the object in a real-time scenario is to have information about its background beforehand and subtract. 

In \cite{yang2004real}, \citeauthor{yang2004real} describe a system for security cameras, able to recognize and tracking a moving object. This is possible once the system is collecting information about the background as time passes. Using the information about the background, the system can subtract any object that considers strange to the background, and start the tracking. This method as the advantage of being fast and discard the possibility of training a classifier for object identification. 
\citeauthor{butler2003real} describe a similar approach in \cite{butler2003real}, where the key is to learn the background and generate a model of it by representing each pixel in the frame by a group of clusters, where these clusters are ordered by the likelihood of modelling the background. Incoming pixels are matched against the corresponding cluster group and classified as part of the background.
In our case, these methods have an obvious problem. Normally, the subject its already placed when the user wants to take a photo, therefore, the subject would be confused as a background or, the user would have to point the camera before placing the subject to initiate a process of extracting information and learn what's the background. When talking about a  camera in a mobile device or any digital camera, this method is nearly impractical, as the regular user tends to move the arms quite easily ruining the learning process started before. Thus, being an unreliable method for object segmentation.

For our use case, we envision a method for generic object segmentation that could be used in real-time. Since it is for generic objects, we couldn't train a classifier to recognize multiple objects, or use edges information for comparison, as it would need multiple photos for data collection, and the segmentation would be limited to a few objects. 

\subsubsection{Algorithm Description}
\label{subsub:seg_algorithm}

As mentioned in \cite{Santos} and \cite{kamps2012rules}, colour is a very important feature in a photo. With this in mind, when photographing, the main subject should cause visual impact due to its contrast relative to the background or other elements in the scenery. For this, we used a slightly simplified version of the \emph{Histogram Based Contrast} (HC) method described by \citeauthor{cheng2011global} in \cite{cheng2011global} for extraction of salient regions in a photo that evaluates global contrast differences, calculating saliency values for image pixels using color statistics. The saliency of a pixel is defined using its color contrast to all other pixels in the image. This can be described as,

\begin{equation}
S(I_{k}) = \sum_{\forall I_{i} \in I} D(I_{k}, I_{i}),
\end{equation}

where $D(I_{k}, I_{i})$ is the color distance between saliency value $I_{k}$ and pixel $I_{i}$ in the input image in the \emph{L*a*b*} color space. Since this computationally expensive, \citeauthor{cheng2011global}, reduces the number of colors needed to consider, by quantizing each color channel to have 12 different values, reducing the number of colors to $12^{3} = 1728$. Since a natural image covers only a small portion of the full color space, the less occurring colors are ignored, ensuring that the most occurring ones, cover at least 95\% of the image pixels. The remaining 5\% are replaced by the closest colors in the histogram. Since this quantization might introduce artifacts, the author preforms a smoothing procedure over each saliency value. Each saliency value is replaced by the weighted average of the $n/4$ neighbours where $n$ is the number of colors that fill 95\% of the image pixels.

After obtaining a saliency map, the map is turned into a binary segmentation mask using a fixed threshold. Finally GrabCut\cite{rother2004grabcut} will be used repeatedly to refine the segmentation result initially obtained by the binary segmentation mask.

\subsubsection{Implementation Details}

To use this algorithm in a real-time scenario in a mobile device, our implementation had to be more simplified with little changes. GrabCut is too heavy to run smoothly on a mobile device, and for that reason, we had to discard its usage sacrificing the refinement of the segmentation for speed. After obtaining the saliency map described in \cite{cheng2011global}, we give a label to each pixel depending on its value. These labels would be used in GrabCut to define which pixels are considered foreground, probable foreground, background, or probable background. In our case, we used the labels to create masks with the areas defined as probable foreground and probable background.
We defined experimentally the thresholds, classifying a pixel as probable foreground if its value was bigger or equal to 200 and probable background if it was between 20 and 200.

After obtaining a mask with each pixel labelled, we created a mask with all pixels that were considered as probable foreground and calculated the center of mass for the resulting mask. To remove artifacts that might exist from using an incorrect threshold value, generate a bounding rectangle that starts at the center of mass calculated and expands in every direction. Each edge will continue expanding while each row or column that passes, has at least a count of 25 pixels belonging to the segmentation mask, and stops when 50 rows or columns have been covered with a count of pixels bellow the previously stated. These thresholds were found experimentally and seem reasonable for the test images.

Obtained the bounding rectangle and mask with areas considered probable foreground, we had a good estimate of what was the main object in the scenario. The result of the segmentation was a section of the mask containing the probable foreground pixels, cropped in the area defined by the bounding rectangle. Since this algorithm is not sensitive to slight variations in lightning, we merged the probable foreground pixels with the probable background ones and cropped with the bounding rectangle. This would later be helpful filling the gaps in the mask, as this algorithm was not sensitive to slight light changes.

\subsubsection{Interface Display}

Generated the mask we chose to simply show a semi-transparent mask over the live feed obtained from the camera, as shown in Figure \ref{fig:interface_segmentation}. This would be a good indicator of what as the subject in the scenario and if it had any color striking features relatively to the rest of the elements.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.2]{interface/interface_segmentation.png}
    \caption{Example of object segmentation interface. The resulting mask of the algorithm is then displayed as a green overlay in the camera live feed.}
  	\label{fig:interface_segmentation}
\end{figure}

This could also be used together with guideline such as Rule of Thirds, described in Section \ref{sub:face_guidelines}, to reposition the object and change the composition.

\subsubsection{Discussion}
\label{subsub:seg_discussion}
Being an incomplete algorithm, it does not work perfectly. Since it depends on the global contrast of a subjects colours, this segmentation might not work if the background is not plain or simple. Even with slight camera shake, this algorithm can extract the subject quite effectively. Figure \ref{fig:seg_example} show the multiple stages taken to segment an object in an image and its final result.

\begin{figure}[htb]
	\hspace*{-45pt}
    \begin{tabular}{cccccccc}
    	\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3.jpg}        &
		\hspace*{-13pt}    	
    	\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_sal.png}    & 						\hspace*{-13pt}
    	\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_bmask.png}    & 					\hspace*{-13pt}
    	\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_pr_bgd.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_pr_fgd.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_rect.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_mask.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/3/re_3_seg.png} \\
    
    	\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2.jpg}    & 							\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_sal.png}    & 						\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_bmask.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_pr_bgd.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_pr_fgd.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_rect.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_mask.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/2/re_2_seg.png} \\
                
    	\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4.jpg}    & 							\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_sal.png}    & 						\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_bmask.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_pr_bgd.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_pr_fgd.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_rect.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_mask.png}    & 					\hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/4/re_4_seg.png} \\
		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10.jpg}        &
        \hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_sal.png}    & 						\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_bmask.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_pr_bgd.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_pr_fgd.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_rect.png}    & 						\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_mask.png}    &
		\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/10/10_seg.png} \\
		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7.jpg}        &
        \hspace*{-13pt}
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_sal.png}    & 						\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_bmask.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_pr_bgd.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_pr_fgd.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_mask.png}    & 					\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_rect.png}    &
		\hspace*{-13pt}		
		\includegraphics[width=0.14\textwidth]{interface/segmentation/7/re_7_seg.png} \\
		a) & \hspace*{-13pt} b) & \hspace*{-13pt} c) & \hspace*{-13pt} d) & \hspace*{-13pt} e) & \hspace*{-13pt} f) & \hspace*{-13pt} g) & \hspace*{-13pt} h)
    \end{tabular}
    \caption{Steps taken when segmenting an object. a) Input image. b) Saliency map generated by the algorithm in \cite{cheng2011global}. c) Binary mask. d) Probable background pixels. e) Probable foreground pixels. f) Bounding rectangle and center of mass point. g) Cropped mask containing the probable foreground area filled in with probable background pixels. h) Segmentation applied to the input image.}
	\label{fig:seg_example}	      
\end{figure}

Has it is possible to observe in Figure \ref{fig:seg_example}, the input image shows a singular object in a plain background with a successful segmentation. The second image it is possible to confirm that, although the object is detected, the foreground threshold is not sensitive to lightning changes, as the darkest side in the can is considered as being a probable background.

This algorithm also might not work when trying to segment two objects. Example of this is the third input image, where we can verify by the saliency map, that the can is obviously more salient than the other object, therefore, the second object is classified as probable background. Another example of this, is the fourth image. With a plain background and two objects, the object with more vivid colours is the one considered as foreground, leaving the remaining object as part of the foreground.

The fifth image shows an example were lightning changes affect the outcome of the algorithm. In this case, the can and a part of the wall are considered as foreground. The solution to this would be a higher threshold when defining the foreground pixels, as the wall is visibly less salient than the can in the saliency map. Since it is detecting the wall as a part of the foreground, this causes the center of mass to be slightly dislocated and generates a bigger bounding rectangle. When merging the probable foreground pixels with the probable background ones and cropping with the bounding rectangle, it segments a big portion of the image that its not relevant.

This object segmentation was also used in other functionalities implemented for this thesis.

\todo[inline]{fazer comparação com dataset utilizado no paper}
\subsection{Image Simplicity}
\label{sub:background}

To reduce the attention distraction by the objects in the background, professional photographers make the background simple. With that perspective in mind, the simplicity of a photo was considered a good feature to analyse.
The most easiest form to obtain simplicity is to place the subject against a neutral background like a backdrop or the sky. Backgrounds can be entirely neutral, like a solid backdrop or a cloudless sky; or they can complement the image, like a starfish on the sand.

Since snapshots often have cluttered backgrounds, professional photos are expected to have edges uniformly distributed in the image, as well, as a subject well defined and in focus. 


\subsubsection{Algorithm Description}

We experimented three algorithms that approach this problem. For our first experiment, we implemented an algorithm described in \cite{kaoautomatic}. Although very simple, the intuition behind this algorithm is that a very complex image, is likely to contain a large amount of edges. In \cite{kaoautomatic} the feature is described as:

\begin{equation}
	B_{complexity} = \frac{n_{e}}{n_{T}},
\end{equation}
where $n_{e}$ is the number of pixels that are edge and $n_{T}$ is the total number of pixels.
 
 
The second algorithm that we tested was presented by \citeauthor{luo2008photo} in \cite{luo2008photo}, were the author explored the simplicity of a photo through its colour distribution.

As described in \cite{luo2008photo}, for a photo, we quantize each color of the RGB channels into 16 values, creating a histogram with 4096 bins, which gives the counts of quantized colors present in the image. After creating the histogram, we calculate the maximum count ($h_{max}$) in the histogram. The simplicity of a photograph is then defined as:
\begin{equation}
	Simplicity = \left(\frac{\|S\|}{4096}\right) * 100%,
\end{equation}
where $S$ is the number of bins in the histogram that equal or bigger than $\gamma h_{max}$. $\gamma$ is 0.01 as chosen by the author in the original article. As described in \cite{luo2008photo}, the simplicity factor for high quality photos fall in $[0\%,1.5\%]$, and low quality photos in $[0.5\%,5\%]$.

In our third experiment we implemented an algorithm described in \cite{ke2006design} by \citeauthor{ke2006design}, were the idea was to compute the spatial distribution of the high frequency edges. Since the snapshots often haven cluttered backgrounds, edges are expected to be uniformly distributed in the image. In professional photographs, the subject is normally well defined and in focus, meaning that high frequency edges will be placed in a smaller area.
We started by implementing a $3\times3$ Laplacian matrix with $\alpha = 0.2$ as follows:
\begin{equation}
\begin{bmatrix}
  0.2 & 0.8 & 0.2 \\
  0.8 & -4 & 0.8 \\
  0.2 & 0.8 & 0.8
\end{bmatrix}
\end{equation}

This is matrix is then applied to the image and take its absolute value to ignore the direction of the gradients. Since this algorithm is to be applied on coloured images in real-time, we split the channels of each frame and perform this computation on each channel, taking the mean across the channels in the end. This will create a Laplacian image that will be resized to $100\times100$ and normalized to values between 0 and 1. This will help when calculating the amount of area the edges occupy. It is expected for well defined objects as the ones used in high quality photos to produce a smaller bounding box, on the other hand cluttered images, are expected to have the oppose effect.
The area of the bounding box is calculated by projecting the Laplacian image $L$ onto de $x$ and $y$ axis independently so that:
\begin{equation}
P_{x}(i) = \sum_{y} L(i,y),
\end{equation}
\begin{equation}
P_{y}(j) = \sum_{x} L(x,j).
\end{equation}

This result in two vectors, one for $x$ axis and another one for $y$, that will have a length correspondent to the image width and height, were each position in the vector will have the amount of pixels considered as an edge in that column or row.

After the projection of the Laplacian image onto the $x$ and $y$ axis, we find the position with the largest count of edges for each vector. The position with the maximum count will be considered the peak and we calculate the width $w_{x}$ and $w_{y}$ for each vector, that contains 98\% of the mass of the projections $P_{x}$ and $P_{y}$. The area of the bounding box containing a high density of pixels is then defined by $w_{x}w_{y}$ and the quality measure for the image is then $1-w_{x}w_{y}$.

\subsubsection{Interface Display}

In our application we tried to understand how these three algorithms would perform under a scenario were this feature would be used in real-time. For that we displayed on the screen the scores obtained from each method were, for the first algorithm\cite{kaoautomatic} and third algorithm\cite{ke2006design}, the score is between 0 and 1, and greater values indicate a simplicity in the scenario. 
For the algorithm presented in \cite{luo2008photo}, can obtain any value between 0\% and 100\% but the author considers a photo simple if the score is between 0.5\% and 5\%. An example of the visualization of these scores can be seen in Figure X.

\todo[inline]{Screenshot a mostrar os scores de simplicidade}

Alternatively, these scores could be replaced a bar that represents the full scale of each algorithm with a marker indicating the current score of the frame.

\todo[inline]{Screenshot a mostrar os scores de simplicidade em barras}

\subsubsection{Discussion}



\subsection{Main Line Detection}
\label{sub:line_detection}

Lines can have an important role in a photograph, as referred in \ref{subsub:leading_lines}. These lines can have multiple interpretations, depending if they are horizontal, vertical, diagonal or curved. They can give a sensation of stability and safety, movement or delimit the begin and the end of a scene.

A popular method for edge detection is the use of Canny Edge Detector\cite{canny1986computational} which can detect the main edges of an image. This method is applies several convolution filters to each pixel with the goal of finding the pixels where the intensity variation is high\cite{nobrega2013interactive}.
The algorithm starts by applying a Gaussian filter which will blur the image reducing the noise and extra edges that might be detected. After the Gaussian filter, it applies two Sobel kernels to find gradients in the horizontal and vertical direction such as:
\begin{equation}
S_{x} =
\begin{bmatrix}
	-1 & 0 & 1\\
	-2 & 0 & 2\\
	-1 & 0 & 1
\end{bmatrix}
,
S_{y} = 
\begin{bmatrix}
	-1 & -2 & -1\\
	0 & 0 & 0\\
	1 & 2 & 1
\end{bmatrix}
\end{equation}
Finally the gradient of a pixel is calculated by
\begin{equation}
	S_{p} = \sqrt{S_{x}^{2} + S_{y}^{2}},
\end{equation}
and the pixel will be accepted as an edge if it is above an upper threshold, below the lower threshold or connected to a pixel that is above the upper threshold.

Another method of line detection used is the through the Hough Transformation\cite{illingworth1988survey}. Lines can be represented in the Cartesian space by the equation
\begin{equation}
	y=mx+b.
	\label{eq:recta}
\end{equation}

Any line can be represented by the equation \ref{eq:recta} and therefore, it can be manipulated to other coordinates such as Polar Coordinates, where its general equation would be
\begin{equation}
	y=\left( -\frac{cos \theta}{sin \theta} \right) x + \left(\frac{\rho}{sin \theta}\right),
\end{equation}
and peach point on a plane is determined by a distance $r$ from a fixed point and an angle $\theta$ from a fixed direction.
The Hough Transform consists in a two-dimensional space where each line is represented by a tuple $(\theta,\rho)$ and therefore all lines that pass through a point $(x_{0}, y_{0})$ can be represented in the Hough Space by the equation
\begin{equation}
\rho = x_{0}cos\theta + y_{0}sin\theta.
\label{eq:hough_eq}
\end{equation}

In Figure \ref{fig:hough_sinusoidal} we can observe the sinusoid obtained by plotting the family of lines that goes through a point $(x_{0}, y_{0})$, in the plane $\theta \rho$. By generating the same plot with a family of lines that pass through a point $(x, y)$ we can find a line by finding the number of intersections between the sinusoid curves has shown in Figure \ref{fig:multiple_sinusoidal}. The more curves intersecting means that the line represented by that intersection has more points\cite{OCV}.
\begin{figure}[htbp]
	\centering
    \subfigure[] {
                \includegraphics[width=0.45\textwidth]{hough_transform.jpg}
                \label{fig:hough_sinusoidal}
    }
    \subfigure[] {
                \includegraphics[width=0.45\textwidth]{multiple_hough.jpg}
                \label{fig:multiple_sinusoidal}
    }
  \caption{a) Sinusoid formed by family of lines that pass through $x_{0} =8$ and $y_{0} = 6$ in plane $\theta r$ b) Plot of three sinusoids that pass through the points $x_{0} = 8,y_{0} = 6, x_{1} = 9,y_{1} = 4, x_{2} = 12, y_{2} = 3$ with an intersection point in $(0.925.9.6)$. This intersection point with parameters $(\theta,\rho)$ defines the line in which $(x_{0},y_{0}), (x_{1},y_{1})$ and $(x_{2},y_{2})$ lay\cite{OCV}.}
\end{figure}

The combination of the Canny Edge detector and the representation of a line in the Hough Space will be useful in the implementation of this functionality.

\subsubsection{Algorithm Description}

In our algorithm we start by applying the Canny Edge detector to each frame. This will result in a binary image with only the edges $B_{e}$ of the frame. In order to get the coordinates with a large count of intersections, we must first create an accumulator with a length equal to the Hough space were the coordinates will be represented. This length will correspond to $180 * 2\rho$, where $\theta$ will have values between $[0,180]$ and $\rho$ will be between $[-\rho,\rho]$.

After the initialization of the accumulator, we scan each pixel of $B_{e}$ searching for a pixel that is edge. When found, the coordinates for this pixel will be used to generate a family of lines in the Polar Coordinates system through the equation \ref{eq:hough_eq}. By varying the $\theta$ value in this equation, we obtain its corresponding $r$ and increment the count of intersections in the coordinates $(\theta,\rho)$.

After filling the accumulator, we select the pairs of $(\theta,\rho)$ with a number of intersections bigger than a given dynamic threshold. To be selected as a line candidate, each pair $(\theta,\rho)$ has to have a number of intersections is bigger than the threshold and be a local maxima. This is verified by comparing its value with the value of its neighbours in a range of $9 \times 9$.

When a line is selected as a main line, we convert its coordinates $(\theta,\rho)$ to the Cartesian Coordinate system. This will result in vector with pairs of points that will later be used to draw a line in the interface, considered as an important line. The steps taken through this algorithm can be visualized in Figure \ref{fig:hough_pipeline}.

\begin{figure}[htb]
	\centering
	\begin{minipage}[b][9cm]{0.5\textwidth}
  		\centering
  		\subfigure[] {	
			\includegraphics[scale=0.3]{interface/mainlines/src.png}
		}
  		\vfill
  		\subfigure[] {
			\includegraphics[scale=0.3]{interface/mainlines/canny.jpg}
		}
  		\vfill
  		\renewcommand{\thesubfigure}{(d)}
  		\subfigure[] {
			\includegraphics[scale=0.3]{interface/mainlines/res.jpg}
		}
  	\end{minipage}
  	\renewcommand{\thesubfigure}{(c)}
    \subfigure[] {
        \label{fig:hough_space}
        \includegraphics[height=9cm]{interface/mainlines/hough.jpg}
    }
	\caption{a) Source image. b) Canny edge detection method applied to the source image. c) Visual representation of the accumulator. Darker areas represent a larger number of intersections in those $(\theta,\rho)$ coordinates. d) Lines drawn in red that represent the main lines detected in the source image.}
    \label{fig:hough_pipeline}
\end{figure}

\subsubsection{Interface Display}

For its interface, we choose to simply draw the lines over the real feed of the camera. The photographer can then use the main lines detected to choose a better angle or rearrange the composition, so that the final viewer has a subliminal guidance when looking at the final product.

Figure \ref{fig:mainline_interface} demonstrates the result of main lines detection applied in a real-time scenario, drawing the prominent lines directly in the viewfinder. In this feature we added controls to increase or decrease the threshold value, that is also shown on the lower left corner.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{\textwidth}
  		\centering
    	\subfigure[] {
    		\includegraphics[width=0.5\textwidth]{interface/mainlines/example1.png}
    	}
  	\end{minipage}
    \subfigure[] {
        \includegraphics[width=0.4\textwidth]{interface/mainlines/example2.png}
    }
    \subfigure[] {
        \includegraphics[width=0.4\textwidth]{interface/mainlines/example3.png}
    }
  	\caption{Main lines detection interface with threshold of 130 (a), 60 (b) and 65 (c).}    				\label{fig:mainline_interface}
\end{figure}

Since a photo can be highly textured, adding control over the number of lines that appear on screen, decreases the probability of having too many lines drawn and can improve the frame-per-second rate at which those lines are shown.

\subsubsection{Discussion}

This algorithm can be quite slow and ineffective if the source image has too many details or textures. A small solution to obtain a speed increase in the algorithm, was to establish a limit of a maximum of 15 lines can be detected at any time for any threshold.

As previously mentioned, this algorithm draws lines across the viewfinder which is a problem of the method used. As an alternative, the method we implemented could be replaced by a line detection method using the progressive probabilistic Hough Transform\cite{matas2000robust}, as this presents line segments instead of lines that go across the full height or width of the image. Although it claims to be faster, it was not chosen as it would be necessary to define even more thresholds that could not be easily controlled by the user. In this case we would have to define the minimum line length and the maximum line gap between points of the same line, which would be very difficult to determine experimentally and would be hard to control these parameters in a mobile device.

In Figure \ref{fig:hough_methods} we can verify the difference between both methods on the same input image with the same parameters (minimum line length and maximum line gap set to default). In Figure \ref{fig:hough_normal} we can verify that it can give the photographer a a more reliable insight of were the lines converge. On the contrary, Hough Transform with progressive probabilistic detection in Figure \ref{fig:hough_P} only shows small scattered segments that make it hard to understand the point of convergence. A solution could be to try and create a more relevant line by joining multiple segments but it would only add computational effort to a device that already struggles dealing with real-time image processing.

\begin{figure}[htbp]
	\centering
    \subfigure[] {
    	\label{fig:hough_normal}
    	\includegraphics[width=0.4\textwidth]{interface/mainlines/hough_example.jpg}
    }
    \subfigure[] {
    	\label{fig:hough_P}
        \includegraphics[width=0.4\textwidth]{interface/mainlines/hough_example_P.jpg}
    }
  	\caption{a) Main line detection with regular Hough Transform. b) Main line detection with progressive probabilistic Hough Transform.}
    \label{fig:hough_methods}
\end{figure}

\subsection{Horizon Detection}
\label{sub:horizon_detection}

Horizon detection in still images or video sequences contributes to applications like image understanding, automatic correction of image tilt and image quality enhancement. Seascapes and landscapes being an usual genre of photography, a feature that detects the horizon line in real-time is useful for a correct placement of the line and indicate a tilt correction on the mobile device. At the pixel level, sky detection can be used for content-based image manipulation, like picture quality improvement using color enhancement and noise reduction, or as background detection for 3D depth-map generation \cite{zafarifar2006blue}.

Sky detection research as also proven useful for object detection for small unmanned vehicles \cite{mcgee2005obstacle}. \citeauthor{mcgee2005obstacle} in \cite{mcgee2005obstacle} presented a system for sky segmentation and horizon detection based on an image colour and texture properties.
For sky segmentation the author used a support vector machine (SVM) for classification of sky and non-sky pixels in the colour space YCrCb, after smoothing the image with a Gaussian filter to reduce the effects of noise. After the SVM divides the sky from non-sky pixels (Figure \ref{fig:h_binary}), a binary image will be generated that will then suffer an erosion and dilation to fill unwanted pixels that were considered sky pixels, as shown in Figure \ref{fig:h_ed}. After removing small sections of misclassified pixels, to find the borders between sky and non-sky regions, it smooths the binary image and classifies all pixels with value near 0.5 as boundary pixels. Performed the edge detection, the horizon detection if then applied using the Hough Transformation method as explained in Section \ref{sub:line_detection}. Found the horizon line, any areas considered as non-sky regions are then considered as an obstacle. Figure \ref{fig:horizon} shows all the steps taken in this algorithm.
\begin{figure}[htbp]
	\centering
    \subfigure[] {
    	\includegraphics[width=0.25\textwidth]{interface/horizon_detection/input.jpg}
    	\label{fig:h_input}
    }
    \subfigure[] {
        \includegraphics[width=0.25\textwidth]{interface/horizon_detection/smoothed.jpg}
    	\label{fig:h_smooth}
    }
    \subfigure[] {
        \includegraphics[width=0.25\textwidth]{interface/horizon_detection/binary.png}
    	\label{fig:h_binary}
    }
    \subfigure[] {
        \includegraphics[width=0.25\textwidth]{interface/horizon_detection/ed.png}
    	\label{fig:h_ed}
    }
    \subfigure[] {
        \includegraphics[width=0.25\textwidth]{interface/horizon_detection/edges.png}
    	\label{fig:h_edge}
    }
    \subfigure[] {
        \includegraphics[width=0.25\textwidth]{interface/horizon_detection/final.png}
    	\label{fig:h_final}
    }
    \caption{a) Input image. b) Smoothed image. c) Binary segmentation. d) Result from erosion and dilation. e) Border between sky and non-sky areas. f) Horizon and obstacle found.}
    \label{fig:horizon}
\end{figure}

%Natsuyasumi
\subsubsection{Algorithm Description}
For our application, adapted part of an algorithm described in \cite{zafarifar2008horizon} that uses both colour and edge features to detect the horizon lines. This algorithm explores the physical phenomenon of color de-saturation and brightness increase along zenith-to-horizon direction to calculate the position and angle of the horizon, that are then refined using edge detection techniques.

We started by implementing part of the algorithm described by \citeauthor{zafarifar2008horizon} in \cite{zafarifar2008horizon} for horizon detection in clear sky. This implementation start by calculating a sky probability map that represents the probability of each pixel belonging to the sky. This probability assumes that clear sky as some properties:
\begin{enumerate}
	\item Pixels in the top area of the image, have a higher probability of being sky-pixels,
	\item They have a certain range of color, when limited to day-light condition,
	\item Pixels that represent sky, contains low texture,
	\item The speed of change in colour values in horizontal and vertical directions is limited,
	\item There is a luminance increase and chrominance decrease along the zenith-to-horizon direction.
\end{enumerate}
Taking into account these properties, the probability of a pixel being a sky-pixel ($P_{sky}$), can be calculated through a pixel colour, texture, position and gradient features by the expression

\begin{equation}
	P_{sky} = P_{colour} * P_{texture} * P_{position} * P_{gradient}.
\end{equation}
In our implementation we simplified the algorithm by ignoring the gradient information and consequently the sky luminance and chrominance variations, which resulted in the expression
\begin{equation}
	P_{sky} = P_{colour} * P_{texture} * P_{position},
\end{equation}
described in \cite{herman2003adaptive}. Has previously stated, the areas that have a higher probability of being related to the sky are conventionally at the top of the image. With that in mind, the probability $P_{colour}$ can be calculated as
\begin{equation}
	P_{position} = e^{- \left( \frac{L}{\#lines} \right)^2},
	\label{eq:colour_sky}
\end{equation}
where L is $y$ for a pixel in the position $(x,y)$, and $\#lines$ is the total height of the image.

For the colour probability distribution is calculated to each one of the channels in \emph{YUV} colour space with the expression
\begin{equation}
	P_{colour} =  e^{- \left[ \left(\frac{y-y_{0}}{\sigma_{y}} \right)^2 + \left(\frac{u-u_{0}}{\sigma_{u}} \right)^2 + \left(\frac{v-v_{0}}{\sigma_{v}} \right)^2\right]},
\end{equation}
where $y$,$u$ and $v$ are the values in each channel of the pixel being processed. The rest of the parameters are determined empirically by the author, as:
\begin{equation}
	y_{0} = 210, \sigma_{y}=130;
	u_{0} = 150, \sigma_{u}=40;
	y_{0} = 100, \sigma_{v}=40.
\end{equation}


The probability function for texture of a pixel is calculated as
\begin{equation}
	P_{texture} = e^{-0.2*(tt)^{2}},
\end{equation}
where $tt$ is the absolute difference of luminance values of a current pixel and the following one in the same line.

Calculated the probability of each pixel with the equation \ref{eq:colour_sky}, we can then segment the probability map and create a binary image using a threshold set as 60, meaning if the pixel as a probability greater than 60\% then it is considered as sky in the binary image.

After segmenting the sky, we apply the Hough Transform to the resulting mask and store the values of $(\rho,\theta)$. These values will be used on the second part of the algorithm that uses edge detection methods.

The second part of the algorithm is were we mix both color and edge detection methods to find the horizon line. From the first part we stored the $(\rho_{CD},\theta_{CD})$ values obtained by applying the Hough Transformation to the binary image containing the areas that were considered sky. The purpose of this is that these parameters indicate the position and angle of the horizon line.

Starting by applying the Canny edge detection method to the luminance component of the image, and since we are only interested in the most prominent straight edges, we choose a large value for the low pass filter. This will smooth the detailed edges yielding in a binary image that contains the significant edges. We then apply the Hough Transformation to the edge map which will result in a matrix representing all the lines in the Hough Space similar to Figure \ref{fig:hough_space}.

Since we previously stored the position and angle of the horizon line calculated by the color edge detection method, we proceed to multiply a weighting factor in the form of a 2D Gaussian function to the result of the Hough Transform in the edge detector with the parameters $(\rho_{CD},\theta_{CD})$ obtained previously, as its center and find the pair $(\rho_{ED}, \theta_{ED})$ with the highest value in the post-processed Hough map.

Now with the pairs $(\rho_{CD}, \theta_{CD})$ and $(\rho_{ED}, \theta_{ED})$ that represent a probable horizon line in each of the methods, we define the horizon line as an average of the ones obtained from the previous methods.

\subsubsection{Interface Display}


\subsubsection{Discussion}

\subsection{Image Balance}
\label{sub:balance}
Many of the photography rules of composition relate to the idea of balance. Ideally, we want our images to be balanced. By "balanced," means that no single area of the image draws our eye so much that we get stuck there. A balanced image feels pleasing to the eye, and not asymmetric in any way or has multiple elements with its visual weight evenly distributed. Every element in your composition carries a certain amount of visual weight. To keep your image balanced, you must compensate for each element with a counter-weight through colours, different levels of contrast, or different subject positions. As described in Section \ref{subsub:balance_elements}, we can easily perceive the balance of an image by imagining a dividing line through the middle and compare the weight of both sides as it can be balanced, unbalanced or have any kind of visual tension between the elements.

\subsubsection{Algorithm Description}

We didn't follow any article in particular as we experimented multiple ways of trying to find the elements in a picture and explore its size. We explain the algorithm for our experiments and the algorithm opted for this feature.

\begin{description}
	\item[First experiment] \hfill \\
	For out first experiment we tried to take a simple approach. The first step was to obtain the left and right part of an image creating two independent regions of the image. Since we were starting by experimenting a way of detecting symmetry, an easy way to achieve that would be by finding the edges of both regions using a Laplacian kernel, and calculate its center of mass. For the center of mass to be close of one another in both regions, one of the sides had to be flipped in the horizontal. After finding the centers of mass we tried to see if the image was symmetric by calculating the Euclidean Distance between both centers of mass.
	
	For a second attempt, instead just dividing the image in left and right side, we divided each of the areas into sic sub-areas and calculate the center of mass of each one of those sub-areas. To see if an image was symmetric we opted to count the number of edges for each subregion and sum the absolute difference between the edge count of overlapping subregions on both left and right side of the image.
	
	\item[Second experiment] \hfill \\
	At this point we started to understand that it was important to properly understand the symmetry axis in an image. In this attempt we started by converting the input frame to the \emph{HSV} colour space and use the hue channel to perform colour clustering. We used implementation of \emph{kmeans} in OpenCV \cite{OCV} to segment the image into two clusters in a naive attempt to segment the object in the image. After the clustering, we find the center of mass of each cluster. We then assume that the perpendicular line between the line segment formed with those two points, is the symmetry axis in the input frame. Found the symmetry axis we calculate the amount of each cluster on each side of the symmetry line. We tried to understand if a image was symmetric by comparing the amounts of each cluster in both sides of the symmetry line.

	\item[Final algorithm] \hfill \\
	In the final algorithm, we used a simplified implementation of a method used for real-time object tracking symmetry described in \cite{li2006real}, as this would give us our symmetry line. To find the symmetry, it starts by converting the input image into an edge image using Canny edge filter with a low threshold of 182 and high threshold of 350, which nearly satisfies the recommendation of a upper:lower ratio of 2:1 \cite{OCV}. Each pair of edge pixels, $i$ and $j$, votes for a particular $\rho$ and $\theta$ in the Hough transform accumulator. In order to reject noisy edge pixels and to favour votes made by edge pixels with symmetric image gradients, a Gaussian weight function is used so that the weight of any pixel $(i,j)$ with the parameters $(\rho,\theta)$ in the Hough space, is maximized when gradient orientations are symmetric about their mid point. This means that for each pair of $(\rho,\theta)$  in the Hough map, its value will be voted by the following equation:

\begin{equation}
	H(\rho,\theta) = \sum_{i,j \in \Gamma(\rho,\theta)}W(i,j),
\end{equation}		
where $\Gamma(\rho,\theta) = \{(i,j) | \rho(i,j) = \rho, \theta(i,j) = \theta\}$.
This means that for a pair $(\rho,\theta)$, its value will be the sum of the weighting function of all pixels $(i,j)$ with the same parameters $(\rho,\theta)$ in the Hough space. The weighting function is described as:
\begin{equation}
	W(i,j) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(\psi_{i}-\psi_{j})^{2}}{2\sigma^{2}}},
	\label{eq:symmetry_eq}
\end{equation}
where $\psi_{i}$ and $\psi_{j}$ are the image gradient angles and $\sigma$ defines the strictness of the gradient symmetry criteria. The relation between the image gradient angles and the level of symmetry is illustrated in Figure \ref{fig:image_balance_fig}. Horizontal and vertical Sobel filters are applied to to determine the image gradients and using the absolute magnitude of angles we verify that $|\psi_{i} - \psi_{j}| = 0$ which means that there is a symmetry, on the other hand if $|\psi_{i} - \psi_{j}| \approx \frac{\pi}{2}$ means that there is no symmetry and therefore, it will receive a very low weight by the equation \ref{eq:symmetry_eq}. Figures \ref{fig:balance1} and \ref{fig:balance2} illustrate these statements.
\begin{figure}[htbp]
	\centering
    \subfigure[] {
    	\includegraphics[width=0.45\textwidth]{interface/image_balance/algo1.jpg}
    	\label{fig:balance1}
    }
    \subfigure[] {
        \includegraphics[width=0.45\textwidth]{interface/image_balance/algo2.jpg}
    	\label{fig:balance2}
    }
    \caption{Angles used in weight calculations. Arrows represent the image gradient direction at the edge pixels. \cite{li2006real}}
    \label{fig:image_balance_fig}
\end{figure}
	\todo[inline]{nao consigo perceber implementação do processo de voto}
	After the voting process, a total of three peaks in the Hough map are selected. The peaks in the Hough accumulator are identified using the Non-Maxima suppression, which locates the highest value above a threshold in the Hough map, selects it as his peak and suppresses all its neighbours to zero. In our case, the suppression neighbourhood of each peak is $\frac{1}{20}\rho_{max}$ and $\frac{1}{20}\theta_{max}$.The lines formed from the three peaks selected, we average the median point of each line segment and slopes. The median point will give us a point in the frame that we know for sure that belongs to the symmetry line and together with the lope, we can calculate which of the frame borders are intersected by the symmetry line.
	
	The next step to take after understanding were is the symmetry line in the input frame, is to calculate the weight of the main elements on each side of the symmetry line. Knowing the symmetry line we then create two masks that will be used to differentiate the left from the right side of the line. We combine these masks with the object segmentation method (Section \ref{sub:segmentation}) obtaining the most salient elements in each one of the sides. 
	
	The final step is to calculate the percentage of amount occupied by the elements on each side of the symmetry line. By calculating the difference between the occupation amount on each side we can then classify as balanced if this difference is bigger or equal to 70\% or unbalanced if the difference is lesser of equal to 30\%. If the difference does not contemplate any of these cases, it is considered that the image as a kind of visual tension.
	
\end{description}

\subsubsection{Interface Display}

Since the purpose of this application is to have simple visual cues, we choose to show the user the line of symmetry being detected at the current frame and in which of the categories it belongs, considering the balance detected. As shown in Figure X, on the upper-left corner, there is a list with the three categories of balance. Given difference percentage of occupation amount between the two sides of the symmetry line, one this categories is highlighted informing the user if the current scenario is balanced, unbalanced or has any kind of visual tension.

\todo[inline]{meter imagem}

\subsubsection{Discussion}

Although it can give an approximate result from what is expected, this feature has some obvious problems on the chosen algorithms. Originally the algorithm described by \citeauthor{li2006real} in \cite{li2006real} was intended to detect and track objects in real-time based on its symmetric features. Thus, if we imagine a plain background with a simple object, the symmetry line will be calculated according its edges, so the line will go across the object. This proves that if an image is completely one-sided, it can be considered as balanced. This would depend on the symmetry axis chosen by the user which is not the case.
This can also bring problems when there is more than one object in the frame, since the lines formed by the chosen peaks, can all be related to the same object. We then added a threshold that gradually increases on each iteration when finding the peaks guaranteeing that the maximum threshold when finding a peak is 
\begin{equation}
	max_{threshold} = max_{threshold} - (max_{threshold}*(0.25*n_{peaks})),
\end{equation}
where $max_{threshold}$ starts with the maximum value found on the Hough map and gradually decreases with the number of peaks found ($n_{peaks}$).

In a scenario with two objects, it becomes possible to select a symmetry axis for each object and calculate an averaged symmetry line with the two previously found. 

Figure \ref{fig:image_balance_example} what was stated. In Figure \ref{fig:balance_example1} is visible the first problem discussed, where the image is obviously unbalanced to anyone who sees the picture but since the are no edges on the right side, the symmetry axis is calculated only with the edges found on the left side. In this case the result is acceptable as it possible to also draw a horizontal symmetry axis making it perfectly balanced, its just counterintuitive. Figure \ref{fig:balance_example2} illustrates the solution we opted for the second problem. Using a dynamic maximum threshold ($max_{threshold}$), we were able to select a symmetry axis for each object and average them, calculating a correct symmetry axis.

\begin{figure}[htbp]
	\centering
    \subfigure[] {
    	\includegraphics[width=0.45\textwidth]{interface/image_balance/example1.jpg}
    	\label{fig:balance_example1}
    }
    \subfigure[] {
        \includegraphics[width=0.45\textwidth]{interface/image_balance/example2.jpg}
    	\label{fig:balance_example2}
    }
    \caption{Examples of an image obviously unbalanced that is considered as balanced due to the symmetry line calculated (a) and the averaging of two symmetry lines detected on two individual objects (b). The blue and red lines represent two of the peaks found in the Hough map and the yellow line, the resulting of averaging them both.}
    \label{fig:image_balance_example}
\end{figure}

Besides the problem in the discovery of the symmetry axis, the object segmentation also has the problems that were discussed in Section \ref{subsub:seg_discussion} where what we consider an object in the scenario, might not have enough color contrast relatively to the background.

The thresholds used in this algorithm were chosen experimentally with sample of images considered symmetric or unbalanced, which means that the values might not perform well on various compositions in a real-time case scenario.


\section{Discussion}
\label{sec:system_discussion}