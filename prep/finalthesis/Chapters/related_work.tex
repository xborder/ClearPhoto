% ===============================================================
% =							Related Work 						=
% ===============================================================
\chapter{Related Work}
\label{cha:related_work}

This chapter will describe concepts, systems, techniques and algorithms related to the theme proposed by this thesis.
This chapter is divided into five sections. In the first section we explain some of the fundamental concepts that are directly related to the camera and its properties. In the second section, we enumerate techniques used to produce high quality images obtained from photography. Since there are many mobile applications related to image capturing and processing, the third section presents a summary of some of those applications during and after the capture. The fourth section, refers to the state of the art related to computational aesthetics and applications of related concepts for image evaluation and classification. The last chapter, describes algorithms for extraction of features in an image.

% ===============================================================
% =						Fundamental Concepts					=
% ===============================================================
% # SECTION: Fundamental Concepts #
\section{Fundamental Concepts of Photography}
\label{sec:concepts}
In the world of photography, there are many technical concepts and properties that are fundamental. It is the photographer's job to use those concepts and properties in order to explore creativity and capture the moment with the best possible result.
For a better understanding of this proposal,  a brief description of some concepts and properties are enumerated in the following sections.

\subsection{Light}
\label{sub:light}
Probably the most fundamental element in photography, capturing the light reflected by the objects is the core in photography. The various colours of the light spectrum are reflected and recorded by the camera sensor, defining an image in a raw format with all the chrominance information.

In photography, it is necessary to understand light, since there are multiple types of light \cite{Santos}. Natural light is easily interpreted as something that emits its own light, and not just reflects it (e.g., the Sun). This light can vary with the seasons, weather conditions and through out the day. Although the source is the same, depending on the season and the time of day, the angle at which the light falls on the subject may vary. Another aspect is the amount of light available that can also vary with the time of day and weather conditions.
Another type of light, is the artificial light, which can be generated by the photographer. It can be manipulated at free will including the number of sources, direction and colour of the light.


\subsection{Exposure}
\label{sub:exposure}

Exposure is the amount of light that reaches the camera sensor and is controlled by choosing the shutter-speed, aperture of the lens and ISO value, although ISO doesn't necessarily affect the amount of light that goes through \cite{kamps2012rules, Santos}. All of these variables are independent and the same result can be obtained by different permutations. The correct combination can have an important part on the end result. Each of these variables will be described later.

\subsection{Shutter}
\label{sub:shutter}

Shutter is a mechanic component that allows the light to pass for determined period of time, and reach the light-sensitive electronic sensor to capture a permanent image of the scene. The velocity the shutter takes to perform an action is called shutter-speed and can vary from seconds to milliseconds, depending on the technique the photographer intends to use to capture the scenery. Lower shutter-speeds allow to create long exposure images, while faster shutter-speeds tend to avoid shaken or blurred images, allowing perfectly sharp images of objects or people in movement \cite{Santos}.

\subsection{Aperture}
\label{sub:aperture}

Aperture is the hole that controls the amount of light that passes and reaches the sensor \cite{kamps2012rules, Santos}. It appears represented in a value of f/, which represent a ratio between the aperture and the focal length, and can be called of \emph{f-stop}. The higher the value, the smaller the aperture value is, and this value will depend on what the subject is and what the photographer wants to maintain sharp in the photo. For example, if the aperture is wide open, then the f/ will be smaller and will result in an image sharpen around what the lens is focusing on and blurred on everything else affecting the depth of field.

\subsection{Depth of Field}
\label{sub:depth_field}

Depth of field represents the portion of the image in front of and behind the focused plan that comes with obvious clarity \cite{kamps2012rules, Santos}. This effect can vary with the lens aperture (Section \ref{sub:aperture}). The larger the lens aperture, the smaller the depth of field which will result in a larger f/ value and a greater amount of light that passes through. Difference between a shallow depth a field and a wider depth of field can be viewed in Figure \ref{fig:depth_field_example}.
The depth of field can be used in a creative manner, leaving to the  photographer's criteria, the amount of sharpness she wants from the nearest object to the farthest object.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:depth_field_example1}
                \includegraphics[scale=0.08]{depth_field_example1.jpg}
    }
    \subfigure[] {
                \label{fig:depth_field_example2}
                \includegraphics[scale=0.08]{depth_field_example2.jpg}
    }
  \caption{Difference between a shallow depth of field (a) and a wider depth of field (b). \cite{kamps2012rules}}
  \label{fig:depth_field_example}
\end{figure}

\subsection{ISO}
\label{sub:iso}

This is the measure that defines the camera sensor sensitivity to the light \cite{kamps2012rules}. Digital cameras tend to behave better in low light conditions with higher ISO values, which means that for higher ISO values, the camera's sensor becomes more sensible to light rays.
In digital cameras and mobile devices, the sensitivity can be adjusted if necessary. However, increasing the camera's sensitivity to the light might ruin a photograph by introducing some digital noise in the image, as shown in Figure \ref{fig:iso_example}\footnote{\url{http://photographylife.com/what-is-iso-in-photography}}. To reduce this negative effect on the image, the use of high ISO values can be compensated with fast shutter speeds and low aperture values.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:iso1}
                \includegraphics[scale=0.45]{iso_example1.jpg}
    }
    \subfigure[] {
                \label{fig:iso2}
                \includegraphics[scale=0.45]{iso_example2.jpg}
    }
  \caption{Difference between an image with ISO value of 200 (a) and 3200 (b).}
  \label{fig:iso_example}
\end{figure}

\subsection{White Balance}
\label{sub:white_balance}

It is a known fact that the human eye is more sensible to light variations than colour variations, therefore, when we see an object reflecting light, our brain instantly interprets the colour. This means that in areas of different brightness, our eyes adapt and interpret the same colour, although, to the camera they are not equal.
Since cameras are not capable of simulating the human brain, that is why white balance is used in photography, in order to match the captured ambience light to what our brain would read \cite{kamps2012rules}. Figure \ref{fig:white_balance_example} illustrates various examples of the same image with different tonalities that can be corrected adjusting the white balance.

\begin{figure}[htbp]
        \centering
    \subfigure[Too cold] {
                \label{fig:white_balance1}
                \includegraphics[scale=0.1]{white_balance1.jpg}
    }
    \subfigure[Well balanced] {
                \label{fig:white_balance2}
                \includegraphics[scale=0.1]{white_balance2.jpg}
    }
    \subfigure[Too warm] {
                \label{fig:white_balance3}
                \includegraphics[scale=0.1]{white_balance3.jpg}
    }
  \caption{Three examples of white balance applied to a photograph \cite{kamps2012rules}.}
  \label{fig:white_balance_example}
\end{figure}


% ===============================================================
% =		    Processing techniques for photography				=
% ===============================================================
% # SECTION: Fundamental Concepts #



% # SUBSECTION: Processing techniques for photography #
\section{Processing Techniques for Photography}
\label{sub:photo_techniques}

To render seascapes showing both the sky and the sea was impossible at the time using standard methods, due to luminosity range being too extreme. Photographers overcame these difficulties by exploring concepts such as the ones described in Section \ref{sec:concepts}. Exploring such concepts led to a development of new techniques, resulting in photographies with different properties. This chapter will describe some of these techniques and how they can be achieved.

\subsection{Long-Exposure Photography}

Long-exposure (or time-exposure) photography exists since the popularization of photography. In the beginning, a person was obligated to stand completely still in front of a camera so that the final result would be as sharp as possible.
With this premise, long-exposure photography is a technique which involves taking a picture with a long shutter-speed \cite{kamps2012rules}. This way the camera sensor will record more light while the shutter is open. With such long speeds the sensor cannot record moving objects, resulting in perfectly sharp capture of stationary objects and blurring or obscuring of moving elements.
This technique is more successful under low light conditions due to the time that the sensor is exposed to light, but this can be suppressed by using special filters for the lenses. By taking so long to close the shutter, the sensor keeps absorbing light creating a brighter photograph producing a near daytime effect.
This technique made easier for professionals to photograph at night, and gave form to new types of photography such as light painting, where a person with a light source can draw paths in the air. Being more sensitive to light, while the shutter is open, the sensor records all the paths drawn resulting in an image where the paths form a continuous line and the person or object moving the light source is obscured, as shown in Figure \ref{fig:long_exposure_example}\footnote{Source: (a) \url{http://www.hongkiat.com/blog/light-painting-artworks/}, (b) \url{http://www.flickr.com/photos/awfulsara/35403447}}. Long-exposure can also be simulated by manually blurring specific areas of a photo, using image editing software.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:long_exposure_example1}
                \includegraphics[scale=0.4]{long_exposure_example1.jpg}
    }
    \subfigure[] {
                \label{fig:long_exposure_example2}
                \includegraphics[scale=0.294]{long_exposure_example2.jpg}
    }
  \caption{Examples of Long-Exposure Photography. \cite{kamps2012rules}}
  \label{fig:long_exposure_example}
\end{figure}

\subsection{High Dynamic Range Imaging}

Although there is a big improvement in the technologies related to photography, cameras still have a problem of not being able to perceive colours the same way as the human eye. Due to the inability of digital cameras to correctly perceive a scene luminosity, it is possible to incorrectly record colours and lose information in lighter or darker areas accordingly to the exposure. High Dynamic Range Imaging is based on a capture that can represent a more accurately range of intensity levels found in scenes, compensating this problem.
In photography, this technique can be achieved by taking multiple Standard Dynamic Range (SDR) photographs of the same scenario with different exposure values that can vary depending on the device. After taking all the samples, the process consists in combining all the raw data of over-exposed and under-exposed areas in one image. By doing this, the image will result in a photograph with a broader tonal range, as shown in Figure \ref{fig:hdr_example}\footnote{Source: \url{http://www.flickr.com/photos/wetworkphotography/7437783578}}.

Although cameras already have enough computational power to perform these techniques, \citeauthor{debevec2008recovering} \cite{debevec2008recovering} also proposed a method of recovering high dynamic range radiance in photographs taken with conventional image equipment. 
As expected, multiple photographs are taken with different amount of exposure. Using these photographs as samples, they recover the response function of the imaging process using the assumption of reciprocity, which can be defined by a sensor response to the total exposure (i.e. $itensity \times time$ controlled by the aperture and shutter-speed (Section \ref{sec:concepts})).
Having obtained a response function, the luminosity value of each pixel is computed using all the available exposures, in which its value is closer to the middle of the response function.

Another approach \cite{vavilin2011fast} involves three cameras, side by side, in the same optical axis. Each of these cameras takes a photograph with different exposure values, taking a first picture underexposed, a second picture with the normal exposure, and a third picture overexposed. These three images are overlapped and merged, reconstructing information lost in overexposed and underexposed areas.

\begin{figure}[htbp]
        \centering
    \subfigure[Non HDR] {
                \includegraphics[scale=0.18]{hdr_example1.jpg}
    }
    \subfigure[HDR] {
                \includegraphics[scale=0.18]{hdr_example2.jpg}
    }
  \caption{Example of a picture taken with (a) Standard Dynamic Range versus (b) the same picture with High Dynamic Range.}
  \label{fig:hdr_example}
\end{figure}

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[scale=0.15]{hdr_camera.png}
%	\caption{Image of the three cameras aligned side by side \cite{Vavilin2011}.}
%	\label{fig:hdr_camera}
%\end{figure}

\subsection{Panoramic Photography}

Panoramic photography is a technique that creates an image with an enlarged field of view which approximates or exceeds the human eye (160º by 75º). Among specialized methods and devices, one of interest is the use of Catadioptric cameras and lenses. These cameras are based on a system of lenses and curved mirrors that allow a field of view of 360º over a single viewpoint, bypassing the need of horizontal panning as it occurs with other methods. Since it uses mirrors and lenses, the light rays bend preventing any kind of distortion or chromatic aberration. Without the need of computation, another advantage is the use of these cameras for video shooting of 360º panoramas. 
There are on the market some add-on lenses for mobiles devices that make this technique possible, such as GoPano micro (Figure \ref{fig:gopano}).

There are also methods to generate panoramas by stitching multiple horizontal images through software.
\citeauthor{brown2007automatic} \cite{brown2007automatic} described an algorithm that would generate a graph to recognize individual panoramas by finding all pairwise image overlaps using feature-based methods. After finding all images with matching features, it would readjust the rotation of the images to generate a panoramic image. This method would be insensitive to the ordering, orientation, scale and illumination of the input images.

\citeauthor{szeliski1997creating} \cite{szeliski1997creating} describe methods to create full view panoramic mosaics. First they describe a method of generation of cylindrical panoramas with a sequence of images taken by a camera mounted on a levelled tripod. This algorithm consists in estimating consecutive horizontal and vertical translations for each image. To recover the translational motion, the incremental translation is estimated by maximizing the corresponding points between them. It would be possible to convert an image to 2D spherical or cylindrical coordinates for a known tilting angle but it would not minimize the error between two images, therefore, this method can only handle the simple case of pure panning motion.

Secondly, \citeauthor{szeliski1997creating} \cite{szeliski1997creating} introduced an algorithm that does not need a set of pure horizontal images. Instead, as long as there is no strong motion between sampled images, there are no constraints on how images are taken. This makes photographs taken by handheld devices without a tripod a reliable source for creating panoramas. According to \citeauthor{szeliski1997creating}, the center point of the sampled images can be described in 3D by a set of matrices which correspond to the image plane translation, the focal length scaling and a 3D rotation matrix. After estimating the mean focal length of the images and rotation matrix, they can stitch the images in a 3D dimensional space.
Since it is made by stitching multiple images, the final product presents distortions at the north pole. This is because of a necessary warp to cylindrical or spherical coordinates (Figure \ref{fig:panoramic_example}) to have a full view of the panorama without using a specialized viewer.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:gopano}
                \includegraphics[scale=0.3]{gopano.jpg}
    }
    \subfigure[] {
                \label{fig:panoramic_example}
                \includegraphics[scale=0.45]{panorama_example.jpg}
    }
  \caption{Image of attachable lens for iPhone, GoPano micro (a), and panorama made by the second algorithm described at \citeauthor{szeliski1997creating} \cite{szeliski1997creating} with distortion at the north pole.}
  \label{fig:panoramic_image}
\end{figure}


% # SUBSECTION: Image capturing and processing apps #
\section{Image Capture and Processing Applications}
\label{sub:capturing_processing}

Since the first attempts to capture a scenery, to its popularization in the XIX century, the world of photography has suffered improvements, that still shock many professionals in the business. Since the upgrade of analogue cameras to the digital world, the use of negatives and dark rooms to new techniques like tilt-shift and HDR imaging, the current market has been increasingly overwhelmed by mobile devices and their ability to easily dethrone today's digital cameras. Proof of this fact is the wide range of applications related to photography available in mobile devices application stores like Play Store and App Store. Some with a more professional objective than others, throughout this chapter it will be discussed some of those applications for image capturing and processing. Along with these applications, research that has been done in this field will also be discussed.

\subsection{Image Capture Applications}

The advancements in mobile devices created a new type of market. Due to this virtual markets available for any Android or iOS user, the number of mobile applications are constantly increasing. Camera related applications are no exception to this rule. In both markets there are many applications fully capable of capturing images that were designed for social networks or include special features. We will start by presenting the default applications in both Android and iOS systems, and other applications found on both of those markets, ending with a discussion comparing all of them.

\subsubsection{Android and iOS Native Applications}

By default, the newest mobile operating systems, already have an incorporated application to take photos. For example, on iOS the default application is rather simple. It has very few customization options for a user that has more knowledge in the area, although it is possible to record videos and choose between full screen photos or photos with a squared format, using one of the two cameras available, with or without flash. Besides these, the iOS native application offers a shortcut to access the device's gallery.

On the other hand, Android's native application is a flexible alternative. It offers access to more advanced functionalities in the likeness of today's digital cameras.
Some of these options include changing ISO and exposure values, white-balancing, contrasts, and choose the resolution of the final product. It also enables the user to choose the correct capture mode for the moment, e.g. sports mode, indoor or portrait.
Android's application adds meta-data tags to the image which may include GPS location and renames the file according to that location. It becomes more user friendly, when it displays a grid on the screen. This grid serves as a guiledine so the user can position the object in the frame. Despite all the options, one of the most useful features is the anti-shake system that applies corrections onto an image to compensate the user's movements.

\subsubsection{Photoshop Express (iOS)}

The tool developed by Adobe \cite{Photoshop} has a shooting mode with some extra features in comparison with the native application.
Having a preview of the image taken is an interesting feature to be used in a more professional context, allowing the user to decide if it is an usable photo before saving it in the gallery.
Although in most of the available applications the zoom feature is already a given, in Photoshop Express it can be controlled by an horizontal slider. The fact that it is always visible, the user understands how to make zoom more easily comparatively to default applications, where the zoom can be done by performing a pinching action on the screen. The pinching action might not be very clear for someone using the application for the first time, therefore, an horizontal slider as the one presented in Photoshop Express might be a good alternative.

\subsubsection{Photosynth (iOS)}

Photosynth, developed by Microsoft \cite{Photosynth}, was created to support a social network centred in creating and sharing panoramic photos. The social features will not be described since they are not the main topic of this thesis.
Regarding the image capturing abilities, this application allows the user to create a software generated panoramic image. The device displays a 3-dimensional spherical space that rotates with the user's movement. As soon as the capture starts, Photosynth automatically captures the initial scene and all the adjacent scenes while the user is rotating. After the capture, this application identifies specific features in one photograph and matches them with others previously taken. Photographs are then paired by analysing the position of their matching features. 
To visualize the panoramic image, the stitched images are displayed in a 3-dimensional spherical space similar to the one presented on the capture display, with the particularity that the user must scroll to see the final result. Outside the application, previewing the image in the gallery, it presents some deformations due to spherical transformations applied to the sampled photos. 


\subsubsection{Camera FV-5 (Android)}

Camera FV-5 \cite{FV5} is one of the most complete applications for photography in the Play Store. Although it has a screen with many options and information, it is what most resembles to a digital camera display. It offers full control over exposure, ISO and white balance. Exposure can be manually selected by the user or, alternatively, she can choose between modes that automatically determine exposure values based specific regions of the image displayed in viewfinder. 
Multiple focusing modes are available, that allow macros, setting the focus to infinity or tapping the display and selecting the object to focus.
More related to camera utilities, multiple flash modes are available including a flash mode that fixes red eyes on photos, and other shooting utilities that include a shooting timer, image stabilization and burst mode.
For a more inexperienced user, default programs with pre-defined exposure settings can be used.
The most interesting trait are the indicators in the viewfinder that display values of exposure time, aperture, ISO, battery remaining and how many photos are in buffer (Figure \ref{fig:FV52}).
Camera FV-5 allows control over the available parameters, recreating some photographic techniques. Due to hardware issues, these recreations are the result of software emulation and not from lens adjustments thus reflecting in the quality of image taken.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:FV51}
                \includegraphics[scale=0.4]{FV51.jpg}
    }
    \subfigure[] {
                \label{fig:FV52}
                \includegraphics[scale=0.45]{FV52.jpg}
    }
  \caption{Interface of Camera FV5 (a) and interface indicator (b) of aperture, exposure time, ISO, etc \cite{FV5}.}
  \label{fig:FV5_image}
\end{figure}


\subsubsection{SketchCam}

SketchCam \cite{labrune2007sketchcam} is a research project that uses a different approach towards mobile devices in photography. With a touch screen, it enables children to capture images by sketching the area of interest on the display. 
Using this approach, it allows the user to become more selective towards the scenario in front of her. It enables creativity in a way that the user may be able to create different frames for the picture that is being taken.  After selecting the point of interest in the view display, it creates an object that can be used for future collages. This may help teaching the basic concepts of composition and photo editing by using a different display. 


\subsubsection{Frankencamera}

Although there are many mobile devices with capabilities to take photos, most of them do not take full advantage of the imaging hardware and offer a highly simplified API. The programmer can't control the camera exposure time or retrieval of raw sensor data. Motivated by these problems, Frankencamera \cite{adams2010frankencamera} is an open-source architecture with a custom-built camera based on Linux and gives full control of the hardware to the programmer through C++ language. These architecture consists in an application processor, a set of photographic devices such as flashes and lenses, and one or more image sensors, each with a specialized image processor, forming a tightly coupled pipeline to coordinate all elements. All sensors, devices and parameters that describe the capture and post-processing of a single output image, can be programmed through its API allowing a mechanism to precisely manipulate the hardware state over time.

Being a custom made platform, it brings some advantages towards closed platforms. One of these advantages is the ability to take long-exposure photos without a tripod. Using an embedded gyroscope, the camera will stream full-resolution raw frames that will be stored, only if their gyroscope tags indicate low motion when the frame was taken. 
Another useful application is the creation of panoramic photos with extended dynamic range. In most devices, the user has to take various individual photographs and stitch them together on a computer, but with this system it is possible to individually set the exposure time of each shot creating a panorama with extended dynamic range and previewing the result instantly.

\subsubsection{Discussion}

All commercial applications and research projects share the most basic features that should come embedded in any system capable of taking photos. These features include access to a gallery, control over flash, control between frontal or rear camera, an auxiliary grid and control over zoom.
Android applications, comparatively to iOS, offer more control over the device's hardware, such as shooting mode, resolution and image quality, aperture and ISO values. Allowing almost full control of the hardware to the user, is a very important feature that must be taken in consideration when developing an application to take photos. Given this fact, Android became a more reliable platform for users that want to use their mobile device for something more than casual photos.

With some interesting features, Camera FV5  is one of those applications for amateur photographers that presents a similar interface to a digital camera. It enables the possibility of adjusting some photographic parameters and introduces the emulation of photography techniques.
Interesting features that should be noted on Photosynth is the way the application handles the creation and preview of panoramas, where it detects and stitches in real-time, a sequence of consecutive photos by matching features on a 3-dimensional spherical space.

As research projects, SketchCam and FrankenCamera can go beyond what is available on standard systems. Although designed for kids, Sketchcam presents a system with a very different way to interact with the user in how she takes a photo. Selecting the point of interest by sketching a continuous path and giving form to different shapes of frames in a display with a live video feed, can be handy when a user only wants to emphasize a region or object in the viewfinder.
Frankencamera allows computational photography to go a step further. It is the perfect example of what is possible by taking full advantage of a device capabilities. It allows to take long-exposure photos using the available gyroscope proving that better photos can be taken using available information from multiple sensors.


\subsection{Image Processing Applications}

Image processing encompasses the task of altering images, whether they are digital photographs, traditional photochemical photographs, or illustrations. During recent years, image retouching of photographs have gained an important role in the industry and more recently, image editing has became available to anyone. Chosen by some as primary cameras, mobile devices with embedded cameras also have a role to play on this subject. Research has shown that users often do little editing of photos after the shot has been taken \cite{brewster2012rethinking}, and for that reason mobile devices now offer applications for both capturing and editing images. This section will describe some applications that apply some image editing principles and tools.
 
\subsubsection{Android and iOS Native Applications}

iOS already brings functionalities for image editing and since it is a system supported by all devices from Apple, automatically all these devices have the some basic tools that can be used in photographs.
These basic tools include image rotating and image cropping where a user can define which part to crop and select the proportion of the rectangle where the crop will be applied.
Since iOS doesn't have an option to remove the red-eye effect while shooting, a user can later remove it by editing the photo and selecting the eye affected. More related to image effects, iOS offers a set of filters that can be applied to a photo including an option of auto enhance, where it readjusts the images white balance and automatically detects and removes the red-eye effect (Figure \ref{fig:ios_edit_image}\footnote{Source: \url{http://www.shortcourses.com/images/b4ch6/eerie.jpg}}).

Android, on the other hand, comparing with previous versions, has a full set of tools available for image processing. Besides sharing the same options as iOS, it has a a group of advanced adjustments that can be applied to a photo \cite{Sharon}. With these, a user can readjust exposure levels, contrast, hue, etc. In cases where only a specific part of a photo needs to be adjusted, Android allows local fine-tuning where a user applies corrections to multiple selected areas. At any point within the editor, a user can drag down from the top to view the original photo and save the edited image specifying the desired size and quality.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:ios_edit1}
                \includegraphics[scale=0.35]{ios_edit1.jpg}
    }
    \subfigure[] {
                \label{fig:ios_edit2}
                \includegraphics[scale=0.35]{ios_edit2.jpg}
    }
  \caption{Image (a) before and (b) after applying the auto enhancing tool. It is possible to see that the red-eye remover only darkens the red area and does not take in account the real colour of the eye.}
  \label{fig:ios_edit_image}
\end{figure}

\subsubsection{PixelNote}

PixelNote \cite{linder2013pixeltone} is an iPad application for photo editing that works through a multimodal interface combining voice recognition and direct user interaction to manipulate images. It uses natural language to express how to modify an image and sketching to localize these changes to specific regions. Using the two inputs, a user can select and tag an object with a voice command that can be used for future identification through voice recognition.

The speech recognition technology converts the voice into character strings that PixelNote can process. First the strings pass through a local speech recognition engine that is trained for a specific set of words selected from a user study. When the system encounters words out of the expected vocabulary, the recorded voice data is sent to a remote speech recognition server. When all else fails, PixelNote shows a gallery with options that may be appropriate . Thus, this fallback system allows the user to also learn the vocabulary of the system while editing the image successfully.

\subsubsection{Discussion}

iOS and Android default applications represent two extremes of what is possible in terms of image editing on a mobile device. Other applications such as Photoshop Express (Android and iOS), Photo Editor (Android) or Camera Awesome (iOS) have similar functionalities as Android's default application. Being relatively advanced in photo editing, Android introduces the notion of local fine-tuning to an image where a user can apply different corrections to localized areas of an image.
As a research project, PixelNote explores different ways of interacting with mobile devices. Using voice recognition with sketching on localized areas, PixelNote enables object selection and tagging, and correction of specific areas in an image by recognizing specific voice commands. This project shows that is possible to extend an application capabilities for photo editing, exploring different types of interaction and reducing the difficulties of such a task on a small and portable screen.

Both multimodal interfaces and localized corrections, are features that should be taken in consideration when thinking of how to apply effects and interact with small screens as the ones in mobile devices.

%------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------------

% # SECTION: Avaliação de Fotografia #
\section{Image Evaluation}
\label{sec:photo_eval}
To understand aesthetic problems, \citeauthor{hoenig2005defining} \cite{hoenig2005defining}1 described and formalized a set of theorems and components that could provide a measurable basis for aesthetics.
According to the author, in 1933, George David Birkhoff came up with a formula that encapsulated his insights into a aesthetic value, described by $ M = Order/Complexity $. This represents the reward one gets, by experiencing orderliness while putting effort in focusing details, giving higher aesthetic value to beauty over complexity. Birkhoff's concept sparked interest of computer scientists in aesthetics, creating the term computational aesthetics. This term is described as a set of computational methods that can make applicable aesthetic decisions in a similar fashion as humans can.

% # SUBSECTION: Regras de Composição #
\subsection{Composition Rules}
\label{sub:photo_rules}

To obtain aesthetic results, photographers follow certain rules of composition that are the result of past artistic development. These rules are now considered as rules of thumb and serve as guidelines. Following these guidelines helps obtaining pleasant results but they are not absolute. Professional photographers also criticize them and defend that one must know when to break them.
Being such a subjective topic, there is no recipe for a good composition and professional photographers always have the final call when taking a photo. This section describes some of the rules that capture the viewers attention and can be used in computational aesthetics.

\subsubsection{Colour Balance}
\label{subsub:colour_balance}

Although a good photograph is dependent on the subject that is being captured, colour has a major impact in creating a certain mood and empathy with the viewer. 

It is rare for a colour to be isolated in a photo shoot, and depending on the colour palette, a different relation between them will be established. These relations can create similar or different emotions that can be explored in a composition \cite{Santos}.
A relation created by two complementary colours gives a sensation of balance, but if both colours have different luminosity values, the less luminous colour must be present in a greater amount comparing to its complement (Figure \ref{fig:colour_balance_image}).
To evoke a mood and arouse emotions, each colour has its own meaning that can be interpreted in different ways by different cultures. For the western civilization, yellow symbolizes cheerfulness, joy and optimism, but for the eastern civilization, it is related to the imperial kingdom and symbolizes something sacred. On the other hand, in Egypt, it is a colour for mourning.

\begin{figure}[htbp]
    \centering
	\label{fig:colour_balance_example}
    \includegraphics[scale=0.3]{color_balance_example.jpg}
  \caption{Image of two complementary colours balanced in a frame \cite{Santos}.}
  \label{fig:colour_balance_image}
\end{figure}

\subsubsection{Rule of the Golden Section and Golden Spiral}
\label{subsub:golden_section}

There are rules used by many photographers, artists, and architects, considered to obtain very appealing results. The golden section rule is based on the golden average. This value can be achieved from a division between two consecutive numbers in a Fibonacci sequence. For example, defining the sequence [8,13,21] as a subsequence of the original Fibonacci, dividing 13 by 8, and 21 by 13 will result in a ratio, that in the limit will be equal to the golden number (i.e. $\approx 1.6180339$). This golden number is what defines a golden section \cite{Santos}. 

This section, which is believed to be aesthetically pleasing, consists of a group of rectangles in which the ratio of the longer side to the shorter is equal to the golden ratio. It is possible to draw a logarithmic spiral whose growth factor is equal to this ratio, called the golden spiral, which converges to the smallest rectangle in the section. 

Using this golden ratio, one can form a grid dividing the golden section in 1.6 to 1 parts. Applying this division to a golden section, we obtain a grid as in to Figure \ref{fig:golden_section2}, where the intersection of the lines indicate imaginary points where the main subject should be located. From this point onward, these imaginary points will be treated as power points. 

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:golden_section2}
                \includegraphics[scale=0.2]{golden_section2.jpg}
    }
    \subfigure[] {
                \label{fig:golden_section1}
                \includegraphics[scale=0.2]{golden_section1.jpg}
    }
  \caption{Guidelines formed from dividing the (a) golden section in 1.6 to 1 parts and from (b) the golden spiral.}
  \label{fig:golden_section_image}
\end{figure}

\subsubsection{Rule of Thirds}
\label{subsub:rule_thirds}

The rule of thirds is based on the golden section \cite{Santos}. The rule of thirds consists in dividing the rectangle in nine equal parts. The scene is divided in thirds both horizontally and vertically with power points at the intersections.
Being a derivation of the golden section, the fundamental concept of where the object should be, remains the same. The main subject should be positioned in one of the power points and along the the lines, as shown in Figure \ref{fig:rule_of_thirds_image}.

\begin{figure}[htbp]
    \centering
	\label{fig:rule_of_thirds_example}
    \includegraphics[scale=0.2]{rule_of_thirds.jpg}
	\caption{Guidelines of the rule of thirds with nine equal rectangles and respective power points at the intersection between horizontal an vertical guidelines.}
	\label{fig:rule_of_thirds_image}
\end{figure}

\subsubsection{Triangles and Golden Triangles Rule}
\label{subsub:rule_triangles}

The most common shape of composition in a portrait is that of a triangle, imagining a portrait with the head being the peak and the width of the body being the base \cite{cleghorn2004portrait}. This enhances the subject and boosts the composition.

The golden triangles rule uses a group of triangles that follow the proportions described by the golden section. Using a golden section, we draw a diagonal line between two corners of the rectangle and connect a perpendicular line to each of the remaining corners. In some cases, this can be simplified to only one perpendicular, having only one power point in the intersection with the diagonal line and a suggestive region in the frame to place the elements \cite{Santos}.

\begin{figure}[htbp]
    \centering
	\label{fig:golden_triangle_example}
    \includegraphics[scale=0.5]{golden_triangle.png}
	\caption{Guidelines obtained by the golden triangles rule.}
	\label{fig:golden_triangle_image}
\end{figure}

\subsubsection{Usage of leading lines}
\label{subsub:leading_lines}

Lines can be used implicitly by creating an imaginary line between two subjects in a picture, or explicitly, like the edges of a building. In the perspective of \citeauthor{kamps2012rules} \cite{kamps2012rules} vertical, horizontal, diagonal or curved lines can be formed of just about anything and have the purpose of leading the viewer to a specific area, giving emphasis to the subject being photographed (Figure \ref{fig:leading_lines_image}\footnote{Source: \url{http://goo.gl/7m0mSS}}).

Horizontal lines are easier to interpret and give a sensation of stability and safety. Vertical lines can delimit the begin and the end of a scene, and work as an enforcement for horizontal lines.
Diagonal lines are responsible for creating perspective in a photo. If the photo does not have a specific subject to photograph, diagonals can direct a viewer to outside of the frame, but on the other hand, the viewer eyes can be imprisoned by using straight angles. 
Curved lines can have a number of angles, and for that reason, they can give a sensation of movement. Although, depending on the subject and depth of field, the same curves might have different results. It is important to refer that these interpretations can depend on the viewer.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:leading_lines1}
                \includegraphics[scale=0.4]{leading_lines1.jpg}
    }
    \subfigure[] {
                \label{fig:leading_lines2}
                \includegraphics[scale=0.4]{leading_lines2.jpg}
    }
  \caption{Examples of a curved line (a) that redirects the viewer's eye to the maple tree, and vertical lines (b) that redirect to the subject in the photo.}
  \label{fig:leading_lines_image}
\end{figure}

\subsubsection{Balance of elements}
\label{subsub:balance_elements}

Unconsciously, the human mind evaluates a photo and checks if there exists any balance or unbalance between the elements \cite{Santos}.
To judge the balance between the elements of a composition, we must imagine a frame divided by two and a scale that will measure the weight of the left side with the right side (Figure \ref{fig:balance_elements_image}\footnote{Source: (a) \url{http://joelsantos.net/}, (b) \url{http://www.flickr.com/photos/victoriagracia/3869143989/sizes/o/}, (c) \url{http://photographyjunction.wordpress.com/2012/07/19/balancing/}
}).
When the scale is perfectly balanced which is very common in symmetrical images, it means that both sides have the same weight visually, creating what is called static balance.
In dynamic balance, it is possible to create a balance in the image between with two imbalanced sides. This can be achieved if one of the sides has a larger element and the remaining side has a smaller but brighter object.
The scale doesn't have to be perfectly balanced and a strong composition can be created with unbalanced sceneries. This way, the viewers attention will lie over the same side, empowering the subject in the frame.


\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:static_balance}
                \includegraphics[width=0.3\textwidth]{static_balance.jpg}
    }
    \subfigure[] {
                \label{fig:dynamic_balance}
                \includegraphics[width=0.3\textwidth]{dynamic_balance.jpg}
    }
    \subfigure[] {
                \label{fig:unbalance}
                \includegraphics[width=0.3\textwidth]{unbalance.jpg}
    }
    \subfigure[] {
                \label{fig:static_balance_exp}
                \includegraphics[width=0.3\textwidth]{static_balance.pdf}
    }
    \subfigure[] {
                \label{fig:dynamic_balance_exp}
                \includegraphics[width=0.3\textwidth]{dynamic_balance.pdf}
    }
    \subfigure[] {
                \label{fig:unbalance_exp}
                \includegraphics[width=0.3\textwidth]{unbalance.pdf}
    }
  \caption{Example images of element balancing and corresponding schematic. (a-d) Static balance normally associated with symmetry. (b-e) Dynamic balance where the left side has a larger subject that is being balanced by a smaller but more brighter subject on the right side. (c-f) Unbalanced picture where the subject is positioned on the left side, leaving the right side with a negative space.}
  \label{fig:balance_elements_image}
\end{figure}

\subsection{Evaluation of Aesthetic Features}
\label{subsub:eval_features}

Some authors have used some of the rules described in Section \ref{sec:photo_eval} as a basis for classifier training or score attribution on extracted features.
Rules like Rule of Thirds (Section \ref{subsub:rule_thirds}) and spatial distribution of a subject have already been topics of research.

\citeauthor{bhattacharya2010framework} \cite{bhattacharya2010framework} attempted to associate a users' notions of aesthetics by formulating photographic quality assessment measures in a machine learning context. One of this measures consists in the relative foreground position which is defined as the normalized Euclidean distance between the foregrounds center of mass and one of the four \emph{power-points}, that although it works for images with single-subject compositions, it is not viable for landscape or seascape scenarios.

\citeauthor{liu2010optimizing} in \cite{liu2010optimizing} uses a similar approach to generate a score. Instead of just using the Rule of Thirds, the authors also use a saliency map and the prominent lines in a photo. The final score is calculated by the Eq. \ref{eq:RT}, where $E_{point}$ represents the sum of the mass of each salient region multiplied by its minimum distance to a \emph{power-point} in the rule of thirds, and $E_{line}$ represents the sum of each saliency of value of a prominent line multiplied by the minimum distance to a rule of thirds line. $\gamma_{point}$ and $\gamma_{line}$ are weights given to each of the components. The author that defined the line based in the Rule of Thirds is a better predictor than its point-based counter part, so the weights in Eq. \ref{eq:RT} are $\gamma_{point} = 1/3$ and $\gamma_{line}=2/3$.
\begin{equation} \label{eq:RT}
	S_{RT} = \gamma_{point}*E_{point} + \gamma_ {line}*E_{line}
\end{equation}

Although it is more oriented for assessing human portraits, \citeauthor{khan2012evaluating} \cite{khan2012evaluating}, presents an approach that explores a photos' spatial composition, by computing a score given the location of a face centroid in a specific template (Figure \ref{fig:khan_template_image}). This template gives higher scores on lighter areas, and good scores on blur locations around those lighter areas.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.4]{spatial_composition_template.png}
	\caption{Spatial composition template used in \cite{khan2012evaluating}.}
	\label{fig:khan_template_image}
\end{figure}


Extracting features related to colour have also been explored when trying to identify a photograph with a good aesthetic score.

\citeauthor{khan2012evaluating} \cite{khan2012evaluating} proposes a set of features extracted from an area where a face is recognized. This set of features include the illumination of a face by calculating the absolute difference between mean Value (in HSV) of left and right side of face bounding box; the background contrast, calculating absolute difference between mean Value of face bounding box and image without face bounding box; and brightness of an image by calculating its mean Value.

Other more elaborated methods were presented in \cite{luo2011content}, where \citeauthor{luo2011content} explores the lightning and color arrangement in a photograph. After identifying the subject area, the author explores the colourfulness, clarity contrast and lightning contrast between the subject area and the background.



\subsection{Practical Application of Aesthetic Evaluation Systems}
\label{sub:eval_applications}

As referred in Section \ref{sec:photo_eval}, computational aesthetics can be described as aesthetic decisions made by computational methods. Increasing aesthetic awareness, researches developed systems and algorithms that extract and evaluate features of an image based in rules such as the ones described in Section \ref{sub:photo_rules}.

Defining aesthetics as a "concern with beauty and art and understanding of beautiful thing", \citeauthor{datta2006studying} \cite{datta2006studying} described a system capable of extracting visual properties and automatically tell the difference between aesthetically pleasing and displeasing images. 
Based on data extracted from an on-line photo sharing community, a set of images and associated aesthetic ratings given by the community were used to train a classifier.

From all image samples, two-dimensional matrices for each of the color components are obtained. A total of 56 candidate features related to the image properties (e.g. exposure, hue, saturation, size and aspect ratio) and composition (e.g. rule of thirds, use of texture and shape convexity) are extracted from those matrices. 
These features were chosen to study patterns that could lead to higher or lower aesthetic ratings. By using a segmentation method based on clustering, information relevant to some features was extracted from objects within the photographs.
From all the candidate features, it was selected a total of 15 visual features that established a significant correlation between the visual properties of photographic images and their aesthetics ratings given by the community. The selected features would later be used by the classifier to attribute a rating to an image.
Later, a publicly accessible system called \emph{ACQUINE} \cite{datta2010acquine} was developed. A user could upload their photographs and have them rated automatically for aesthetic quality. Compromising on a subset of the features previously presented, \emph{ACQUINE} was able to generate quick responses through a simple interface (Figure \ref{fig:acquine_interface1}) that kept the underlying classifier hidden. A user would then submit an image and wait for the classifiers prediction of aesthetic value. The user could also give a rating on a 7-star scale (Figure \ref{fig:acquine_interface2}) (similar to Photo.net's rating scale) that would be stored for future validation and improvements on the classifier.

\begin{figure}[htbp]
        \centering
    \subfigure[] {
                \label{fig:acquine_interface1}
                \includegraphics[width=\textwidth]{acquine_interface1.jpg}
    }
    \subfigure[] {
                \label{fig:acquine_interface2}
                \includegraphics[width=\textwidth]{acquine_interface2.jpg}
    }
  \caption{(a) The frontpage of the ACQUINE system, where the users could upload photos. The list of top users on the right side and photos with high ACQUINE aesthetics scores randomly selected at the bottom. (b) Screenshot of the ratings page after a photo upload. \cite{datta2010acquine}}
  \label{fig:acquine_image}
\end{figure}

Following the same methodology, \citeauthor{yeh2010personalized} \cite{yeh2010personalized}, described a mutable ranking system. This system listed 1000 ranked photographs ordered from the highest rank to the lowest. The score of each photograph was considered as a linear combination of each feature and its corresponding optimal weighting factor, that was found after the extraction of features. However, since the optimal weights might not combine with the users preferences, it allows them to combine personal taste with a trained model, and rearrange the ordered list of ranked photographs.
The weighting adjustments can be feature-based where the user can personally select the weight of each feature (Figure \ref{fig:system_eval1}), or an example-based approach (Figure \ref{fig:system_eval2}), where the user selects a photograph of their liking and the system updates the weighting based on the example chosen.
The features chosen to extract are quite similar to the ones used in \cite{datta2006studying}, but introduces an interesting composition rule. They extract the subject region and assess the simplicity of a photograph by the colour distribution of the remaining region that corresponds to the background.

\begin{figure}[htb]
    \centering
    \subfigure[] {
                \label{fig:system_eval1}
                \includegraphics[scale=0.25]{system_eval1.jpg}
    }
    \subfigure[] {
                \label{fig:system_eval2}
                \includegraphics[scale=0.25]{system_eval2.jpg}
    }
  \caption{(a) Re-ranking photographs by adjusting the feature weighting and (b) by selecting a few photographs as example \cite{yeh2010personalized}.}
  \label{fig:system_eval_image}
\end{figure}

\section{Computer Vision Algorithms}

Feature detection and matching are an essential component of many computer vision applications. Feature detection refers to information extraction from an interesting part of the image that can be later used for matching and finding a correlation between other images. In this chapter we will describe different types of features that can be used to determine different relations between images.

\subsection{Keypoint Detection}
\label{sub:keypoint}

Keypoint detection is based on specific locations in the images that can be easily distinguished. These local features are often described by the appearance of patches of pixels surrounding the point location \cite{szeliski2011computer}.
There are several feature detectors of this type. In this section, we will describe three: SIFT \cite{lowe1999object}, SURF \cite{bay2006surf}, and FAST \cite{rosten2006machine}.

\subsubsection{SIFT}
\label{subsub:sift}
Scale Invariant Feature Transform (SIFT) \cite{lowe1999object}, is an algorithm used for object recognition. This algorithm uses a set of features that are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes, noise and minor variations in the viewpoint.
The algorithm starts by identifying stable points that remain invariant to scale transformations. After extracting the first keypoints, eliminates the ones that have low contrast and are badly localized alongside an edge, narrowing the total number of keypoints. Ensuring that the keypoints are more stable for matching and recognition, dominant orientations are assigned to each keypoint based on local image gradient direction. A descriptor vector for each of the remaining keypoints is computed in the final stage.

\subsubsection{SURF}
\label{subsub:surf}

Speeded Up Robust Features (SURF) \cite{bay2006surf} is an algorithm that detects points of interest that are scale and rotation invariant while claiming to be faster and more robust than SIFT (Section \ref{subsub:sift}). This algorithm uses a Hessian matrix with the convolution of Gaussian functions for feature detection. The SURF descriptor reproduces the orientation based on a circular region around a point of interest and constructs a square region aligned to the selected orientation extracting the descriptor from it.

\subsubsection{FAST}
\label{subsub:fast}

Features from Accelerated Segment Test (FAST) \cite{rosten2006machine} is a fast algorithm capable of detecting feature points in real-time frame-rate applications. FAST is a corner detector algorithm that detects a candidate point and tests if it is a corner through its adjacent points inside a perimeter of 16 pixels. Although it is faster than both SIFT (Section \ref{subsub:sift}) and SURF (Section \ref{subsub:surf}), FAST is not as robust. The reduced ability to average out noise is why the results are not as good as the other algorithms, but can be used to process image features in real-time.

\subsection{Colour Features}

Colour also provides valuable information for object description and matching. However, there can be large variations in lighting and viewing conditions, complicating the description of images. Therefore, the properties of colour features extracted must be invariant.
SIFT (Section \ref{subsub:sift}) has proven to be a very robust and precise feature descriptor, but properties such as light color changes have no effect because the image is converted to gray-scale \cite{van2008comparison}. \citeauthor{abdel2006csift} \cite{abdel2006csift} extends this algorithm by proposing Coloured SIFT (CSIFT), building the SIFT descriptor in a colour invariant space.
\citeauthor{bosch2007representing} \cite{bosch2007representing} also presented an algorithm that computes SIFT features over all three channels of the HSV color model. However, for this algorithm, only S is invariant to light color changes. H and S are only invariant to scale and shift transformations.

\subsection{Texture Features}

Texture is considered an important component of human visual perception. Texture can be defined by its coarseness, contrast and direction, and has properties such as periodicity and scale \cite{howarth2004evaluation}.
\citeauthor{manjunath1996texture} \cite{manjunath1996texture} presented a method for extraction of texture information for browsing and retrieval of large image data. This method uses a Gabor function that results in the mathematical representation of a sinusoidal wave. This function is then applied multiple times over the image with different parameters, changing both the scale and orientation of the sinusoidal. Working as a filter, the remaining pixels are then eligible as candidate features. The values of scale and orientation applied on the Gabor function to obtain each candidate feature are then used as its feature descriptors.

\subsection{Discussion}

Of the three algorithms described for keypoint feature detection, SIFT is the most robust one. SIFT provides better results and SURF and FAST are faster, sacrificing its robustness. For the context of this dissertation, FAST might be the most appropriate as it claims to be able of detecting features on real-time frame-rate applications.

Because color features are often computed around specific salient points, they cannot be evaluated independently. For a more discriminative power, one must have in consideration the invariance properties of color features. In this section we described methods that use different color spaces to explore such invariances.

Textures can be defined by its coarseness, contrast and direction, and has properties such as periodicity and scale. In this section we presented Gabor functions as one of the methods used to extract texture information from an image.